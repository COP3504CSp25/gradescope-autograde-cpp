#!/usr/bin/env python3
import grade_util as gu
import bootstrap
import shutil
import os
import stat
import glob
import yaml
import json
import subprocess
import unittest
from typing import List

from gradescope_utils.autograder_utils.decorators import weight
from gradescope_utils.autograder_utils.json_test_runner import JSONTestRunner

ZERO_LEADERBOARD = [
    {
        'name': 'Score',
        'value': 0
    }
]

def ZERO_RESULT(msg):
    return {
        'score': 0.0,
        'stdout_visibility': 'hidden',
        'output': msg,
        'leaderboard': ZERO_LEADERBOARD
    }

def BAD_FORMAT(file):
    return ZERO_RESULT('Expecting the file \'{0}\', either no .h file was submitted or filename was not \'{0}\'.'
            .format(file))

def SUBMISSIONS_EXCEEDED(limit):
    return ZERO_RESULT('Exceeded maximum number of submissions: {}'.format(limit))

def NUM_SUBMISSIONS_INFO(num, limit):
    return ZERO_RESULT('Submission {} out of {}'.format(num, limit))


#  class used in generating unittest TestCase's
class Test(type):

    def __new__(mcs, test, bases, attrs):
        attrs[test.__doc__] = test
        return super(Test, mcs).__new__(mcs, test.__doc__, bases, attrs)


def get_test_dirs() -> List[str]:
    '''
    Returns a list of directory paths, as strings, for each test contained in 'tests' directory
    '''
    return [dr[:-1] for dr in glob.glob(gu.Config.TEST_DIR + '/*/')]


def load_yaml(file: str):
    with open(file, 'w+') as f:
        return yaml.safe_load(f) or {}


def generate_test(dir_name):
    settings = load_yaml(os.path.join(gu.Config.TEST_DIR, dir_name, 'test.yml'))

    def run_test():
        run_path = os.path.join(gu.Config.TEST_DIR, dir_name, 'run_test')
        if gu.file_exists(run_path):
            os.chmod(run_path, os.stat(run_path).st_mode | stat.S_IEXEC)
            subprocess.check_call(['dos2unix', 'run_test'], cwd=os.path.join(gu.Config.TEST_DIR, dir_name),
                                  stdout=gu.FNULL)
            subprocess.check_output(['./run_test'], cwd=os.path.join(gu.Config.TEST_DIR, dir_name),
                                    stderr=subprocess.STDOUT, timeout=settings.get('timeout', None))
        else:
            raise Exception('Test improperly configured: missing \'run_test\' executable')

    @weight(settings.get('weight', 1))
    def wrapper(self):
        show_output = settings.get('show_output', True)
        try:
            run_test()
        except subprocess.CalledProcessError as e:  # test script returned non-zero
            msg = '{}\n\n{}'.format(settings.get('message', ''),
                                    e.output.decode() if show_output else '')
            raise Exception(msg)
        except subprocess.TimeoutExpired as e:
            msg = '{}\n\n{}'.format('Test timed out', e.output.decode() if show_output else '')
            raise Exception(msg)

    wrapper.__doc__ = '{}'.format(settings.get('name', os.path.basename(dir_name)))
    return wrapper


def run_tests(pending_messages):
    suite = unittest.TestSuite()

    #  go through all directories in /tests
    for dr in get_test_dirs():
        test_fn = generate_test(dr)
        t = Test(test_fn, (unittest.TestCase,), {})
        suite.addTest(t(test_fn.__doc__))

    with open(gu.Config.RESULTS_FILE, 'w+') as result_stream:
        JSONTestRunner(stdout_visibility='hidden', visibility='visible', stream=result_stream).run(suite)
        result_stream.seek(0)
        data = json.load(result_stream)
        final_score = data.get('score', 0.0)
        data['leaderboard'] = [{'name': 'Score', 'value': final_score}]
        if 'tests' not in data:
            data['tests'] = []
        for msg in pending_messages:
            data['tests'].insert(0, msg)
        result_stream.truncate(0)
        result_stream.seek(0)
        json.dump(data, result_stream)


#  Execution begins here
if __name__ == '__main__':
    bootstrap.init()

    pending_messages = []
    config = load_yaml('config.yml')

    limit = config.get('limit_submissions', -1)
    if limit is not -1:
        num_submissions = gu.number_submissions()
        if num_submissions > limit:
            gu.write_result(**SUBMISSIONS_EXCEEDED(limit))
            exit(0)
        else:
            pending_messages.append((NUM_SUBMISSIONS_INFO(num_submissions, limit)))

    for file in config.get('required_files', []):
        #  check if required file is submitted
        if not gu.is_submitted(file):
            gu.write_result(**BAD_FORMAT(file))
            exit(0)
        #  copy submitted file to shared test directory
        shutil.copy(os.path.join(gu.Config.SUBMITTED_SOURCE, file), gu.Config.TEST_DIR)

    run_tests(pending_messages)
